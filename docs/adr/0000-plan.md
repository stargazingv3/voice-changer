Love it — here’s a single, end-to-end roadmap that fuses the best parts of all three approaches, focused on getting a **Windows-first, real-time, game-usable voice changer** working quickly, with a clean path to a full product.

---

# 0) What “working” means (MVP acceptance criteria)

* **Usable in Discord / games** via a **virtual microphone**.
* **Two selectable pipelines** in the client UI:

  * **Low-latency Voice Conversion (VC)** for gaming/VoIP (target < 60–80 ms round-trip on a local GPU server).
  * **ASR→TTS** “Max Privacy” mode (target < 200–300 ms; acceptable for non-competitive use).
* **Client app is a signed Windows executable** (no Python envs for the user).
* **Server runs in Docker** (dev on WSL2; deployable to AWS GPU).
* **Monorepo** with working CI builds for client installer + server image.

---

# 1) Architecture (practical + future-proof)

**Client (Windows native app)**

* Responsibilities: audio capture (WASAPI), VAD/denoise, buffers/jitter, transport, preset control, status UI, and writing to a **virtual mic**.
* Stack: **Tauri** (Rust backend + web UI) or **Qt/C++**. Recommend **Tauri** for cross-platform runway and compact installers.

**Server (Docker, GPU-ready)**

* Responsibilities: real-time inference for **VC** and **ASR→TTS**, session management, model routing, observability.
* Stack: **Python** (FastAPI for control plane + **gRPC bidi** for audio streaming after MVP), **PyTorch/TensorRT** for models, **NVIDIA Container Toolkit**.

**Transports (phased)**

* **Phase A (MVP):** WebSockets for simplest end-to-end streaming and debugging.
* **Phase B:** **gRPC bidirectional streaming** for LAN/local Docker stability.
* **Phase C:** **WebRTC** for remote/cloud with jitter/NAT handling.

**Virtual Microphone (phased)**

* **Phase 1:** Ship with **VB-CABLE** and one-click setup from the client.
* **Phase 2:** WASAPI loopback fallback profile for edge cases.
* **Phase 3:** Your **own signed virtual mic driver** (WDK, EV cert, WHQL).

**Pipelines**

* **VC pipeline** (low latency): Mic → DSP/VAD → content/speaker encoder → timbre/style mapping → neural vocoder → out.
* **ASR→TTS pipeline** (privacy-max): Mic → VAD → streaming ASR → text → low-latency TTS (pre-warmed voice) → out.

**Latency budgets (targets)**

* Capture + DSP: 5–15 ms
* VC inference + vocoder: 20–50 ms (TensorRT-optimized, small hop sizes)
* Network (local): 5–15 ms
* **VC total:** 40–80 ms
* **ASR→TTS total:** 150–300 ms (depends on chunking and model sizes)

---

# 2) Tech choices (today) and how they evolve

**Client**

* **Audio I/O:** WASAPI (exclusive mode option), ring buffers with backpressure metrics.
* **UI:** Tauri webview with Svelte + Rust commands for audio/transport.
* **DSP:** RNNoise-style denoiser or similar CPU-light denoise; VAD threshold controls in UI.

**Server**

* **Inference:** PyTorch now; **TensorRT** engines for the production VC/vocoder path.
* **ASR:** streaming-capable (Vosk/Coqui or a streamed Whisper variant).
* **TTS:** **Piper** or **Coqui TTS** tuned for low latency; pre-warm voice presets.
* **Router:** simple Python scheduler; later: Ray or custom pool manager.
* **Observability:** Prometheus metrics + OpenTelemetry traces; per-stage latency histograms.

**Optional (later)**

* **Agentic orchestrator:** watches p95 latency, packet loss, and autoswitches chunk sizes or pipeline (VC↔ASR→TTS) per user policy.
* **MCP exposure:** your server’s knobs (voices, models, routes) exported as tools for dev automation and QA.
* **Mojo:** explore for hot DSP kernels server-side after bottlenecks are measured.

---

# 3) Monorepo layout (Cursor-friendly)

```
Voice Changer/
  client/                      # Windows UI + audio
    app/                       # Tauri UI (src, assets)
    core/                      # Rust audio engine, transport, buffers
    virtual_mic/               # driver assets & installer (Phase 3)
    packaging/                 # NSIS/WiX configs, code signing
  server/                      # Dockerized inference
    api/                       # control plane (FastAPI), auth, presets
    stream/                    # audio streaming endpoints (WS, gRPC)
    pipeline/                  # VC and ASR→TTS pipeline runners
    models/                    # model loaders, TRT engines, caching
    config/                    # model & pipeline configs, voices
    observability/             # metrics, logs, tracing
    Dockerfile
    docker-compose.yml         # dev stack (CPU/GPU profiles)
  shared/
    proto/                     # protobuf/IDL (for gRPC phase)
    messages/                  # schemas if using JSON/Flatbuffers
    presets/                   # JSON voice & DSP presets
  tools/
    ci/                        # GH Actions workflows
    scripts/                   # dev scripts (just/make/pwsh)
  docs/
    adr/                       # Architecture Decision Records
    runbooks/                  # ops, release, driver signing
  README.md
```

> Keep everything in **one repo**. Protocols/config live in `/shared` so client and server stay in lockstep.

---

# 4) Development environment (using **Cursor** on Windows + WSL2)

**On Windows (host)**

* Install: Cursor, Rust toolchain, Node (for Tauri UI), NSIS/WiX (packaging), VB-CABLE (dev use), Git, Visual Studio Build Tools (for C++ toolchain).
* Audio test tools: EarTrumpet, LatencyMon, OBS.

**WSL2 (Ubuntu)**

* Docker + NVIDIA Container Toolkit (if you have an RTX).
* Python env (inside Docker only), make/just, jq.

**Recommended Cursor workspace**

* Open the monorepo at root.
* Use Cursor’s tasks for:

  * `dev:client` (run Tauri dev build)
  * `dev:server` (docker-compose up)
  * `lint/test` pipelines
* Extensions: Docker, Markdown, YAML, GitHub Actions, Rust Analyzer, Tailwind CSS (if using), Python (for server editing only).

---

# 5) CI/CD & releases (practical now, scalable later)

**GitHub Actions**

* **Build matrix**:

  * Windows: build & sign Tauri executable → produce installer (NSIS/WiX).
  * Linux: build & push **GPU** Docker image (CUDA version tagged).
* **Artifacts**: attach Windows installer + Docker image digest to GitHub Releases.
* **Versioning**: SemVer; embed protocol version in client & server.
* **Channels**: `nightly` (auto-update opt-in) and `stable`.

**Signing & AV hygiene**

* Code signing cert for the client (reduces AV false positives).
* Later: EV cert for driver (Phase 3).

---

# 6) Milestones (fastest path to “it works”)

### **M0 – Skeleton & pass-through (1 week)**

* Monorepo scaffold + `docker-compose` with a stub server (echo audio back).
* Client: WASAPI capture → send (WS) → receive → play to speakers; basic UI with device selector.
* VB-CABLE install helper + “Set Output Device” wizard.
* **Definition of done:** You can speak and hear yourself via the pipeline with stable buffers and meters.

### **M1 – Real-time VC (2–3 weeks)**

* Implement **VC pipeline** on server; optimize small hops/windowing; warm vocoder.
* Client shows **latency bars per stage** + jitter indicators.
* Output to **VB-CABLE** so Discord/Steam sees “Voice Changer Mic.”
* **Definition of done:** In Discord test call, others hear the converted voice with minimal lag and stable quality.

### **M2 – ASR→TTS mode (1–2 weeks)**

* Add streaming ASR + low-latency TTS; chunking with partials; cross-fade between chunks.
* UI toggle “Max Privacy (ASR→TTS).”
* **Definition of done:** Conversational use feels acceptable (<300 ms); transcription not saved unless opted in.

### **M3 – Hardening & packaging (1–2 weeks)**

* Client crash reporting, logs; in-app updater; signed installer.
* Server GPU image build; startup pre-warm of top N voices.
* Observability: Prometheus metrics dashboards incl. **end-to-end p95 latency**.
* **Definition of done:** One-click installer works on a fresh Windows 11 machine; server runs with `docker run` on a single GPU.

### **M4 – Transport upgrades & scale (2 weeks)**

* Move streaming to **gRPC bidi** for LAN; add **WebRTC** for remote.
* Add simple **session scheduler** (concurrency caps per GPU; back-pressure).
* **Definition of done:** AWS g5.xlarge instance can host multiple concurrent sessions with stable latency.

### **M5 – Virtual Mic Driver (parallel, 4–6+ weeks)**

* WDK project, EV code signing, WHQL. Installer integrated into main setup.
* **Definition of done:** “Voice Changer Mic” appears automatically, no third-party tools.

### **M6 – Orchestration polish (optional)**

* “Smart Mode”: automatically switches VC↔ASR→TTS based on user latency/packet loss policy.
* MCP endpoints for QA automation and scripted load tests.

---

# 7) Product surface (what the user sees)

**Main UI**

* Device pickers (input/mic, output/virtual mic), voice preset dropdown.
* Mode selector: **Ultra-Low Latency (VC)** / **Balanced** / **Max Privacy (ASR→TTS)**.
* Live meters: input level, output level, round-trip latency, packet loss.
* Quick-switch target apps: one-click helper to set Discord/Steam to “Voice Changer Mic.”
* Safety toggles: watermark synthetic audio, show “synthetic” status indicator.

**Settings**

* VAD sensitivity, denoise strength, gain; TTS speech rate/energy; pitch bias.
* Server location selector (local Docker vs cloud region).
* Privacy: opt-in transcripts, data retention (default: none).

---

# 8) Testing playbook (keep it lean)

* **Audio correctness:** sine sweep & speech samples; check clipping, DC offset, and phase.
* **Latency:** loopback test with click-track; measure p50/p95 end-to-end.
* **Network robustness:** throttle to 5–10% packet loss; verify jitter buffers.
* **Game/VoIP interop:** Discord test call, CS/Valorant/Steam voice chat.
* **Cold start:** server GPU cold boot to first audio out — keep < 2–3 s via pre-warm.
* **Security:** verify no raw mic audio leaves the client when “local-only” is selected.

---

# 9) Risks & mitigations

* **Latency spikes** → Smaller frames, pin threads, real-time priority on client; TRT engines server-side.
* **Model drift/quality** → Ship presets with locked engine versions; golden audio comparisons in CI.
* **VB-CABLE friction** → Wizard + auto-config; clear fallback (speakers) with warnings.
* **Driver project scope** → Treat as parallel track with its own budget; don’t block product release.

---

# 10) What to build **first week** (so momentum is real)

1. Create monorepo with the structure above.
2. `docker-compose up` a stub server that timestamps and echoes audio.
3. Tauri app with device selection + WASAPI capture/playback; live level meters.
4. Integrate VB-CABLE and confirm Discord sees converted output (even if pass-through at first).
5. Add a build workflow in GitHub Actions that outputs a **signed Windows installer** and a **server image**.

---

## Final notes on tools & choices

* **Cursor** is perfect as your primary IDE. You only need **Visual Studio + WDK** when you do the custom virtual mic driver (Phase 3), which still lives in the **same repo**.
* Keep Python **only on the server** (inside Docker). Client stays **Rust/C++** for performance and clean installers.
* Stay disciplined with **ADRs in `/docs/adr`** so future you remembers *why* a decision was made (e.g., WebSockets first, then gRPC).